import os
import csv
import re
import asyncio
import logging
from datetime import datetime
import anthropic
import html
from asyncio import Semaphore
from concurrent.futures import ThreadPoolExecutor

# Configure logging
log_filename = f"log_score_answers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename),
        logging.StreamHandler()
    ]
)
# Configuration
CONFIG = {
    # Scoring LLM configuration (Claude API)
    "SCORING_LLM_MODEL": "claude-3-5-sonnet-20240620",  # currently we are using claude 3.5 sonnet as our LLM testing engine
    "CLAUDE_API_KEY": os.getenv("ANTHROPIC_API_KEY"),  # Claude API Key

    "ANSWERS_CSV": "answers.csv",  # answers CSV that generated by testing engine
    "OUTPUT_CSV": f"scored_answers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",

    # Concurrency settings
    "MAX_CONCURRENT_SCORING": 3,    
    "MAX_CONCURRENT_API_CALLS": 6, 

}

CONFIG["SCORING_PROMPT_QA"] = """**Role: You are an expert evaluator in blockchain technology and smart contract development, tasked with rigorously assessing a student's answer. Your evaluation should be based solely on the provided question and the standard answer, focusing on accuracy, depth, and clarity.

Materials:

Question: {question}
Standard Answer: {standard_answer}
Student's Answer: {student_answer}
Evaluation Criteria (Total 100 points):

Understanding and Application of Concepts (40 points):

Excellent (36-40): Demonstrates a comprehensive understanding of all relevant concepts and applies them correctly.
Good (28-35): Shows a solid grasp with minor misunderstandings or omissions.
Fair (20-27): Displays basic understanding with noticeable gaps.
Poor (0-19): Lacks understanding of key concepts; significant errors.
Accuracy and Completeness (30 points):

Excellent (27-30): Provides accurate and complete information, addressing all aspects of the question.
Good (21-26): Mostly accurate with minor errors; addresses most parts.
Fair (15-20): Some inaccuracies; incomplete coverage.
Poor (0-14): Numerous inaccuracies; fails to address key elements.
Critical Thinking and Insight (20 points):

Excellent (18-20): Exhibits deep analysis and original insights beyond the standard answer.
Good (14-17): Shows some analysis and understanding of implications.
Fair (10-13): Limited analysis; relies heavily on surface-level information.
Poor (0-9): Lacks critical thinking; purely restates information.
Clarity of Communication (10 points):

Excellent (9-10): Articulates ideas clearly and logically with excellent language use.
Good (7-8): Generally clear with minor language issues.
Fair (5-6): Some clarity but with noticeable language or organizational problems.
Poor (0-4): Difficult to understand; poor language and organization.
Instructions:

Provide a specific score for each criterion based on the student's performance.
Justify each score with concise feedback, highlighting strengths and areas for improvement.
Be objective and critical, ensuring that the evaluation reflects the true quality of the answer.
Avoid bias and base your assessment solely on the content provided.
Response Format:
Total Score: X/100

1. Understanding and Application of Concepts (X/40):
[Your feedback here]

2. Accuracy and Completeness (X/30):
[Your feedback here]

3. Critical Thinking and Insight (X/20):
[Your feedback here]

4. Clarity of Communication (X/10):
[Your feedback here]

**Overall Comments:**
[Summarize the student's performance, highlighting key strengths and areas for improvement.]


"""


CONFIG["SCORING_PROMPT_AUDITING"] = """**Role: You are a senior smart contract auditor with extensive experience in blockchain security. Your task is to meticulously evaluate the student's analysis of a given smart contract code. Base your evaluation solely on the provided materials, focusing on the identification of vulnerabilities, correctness of explanations, and the depth of security insights.

Materials:
Question: {question}
Code: {code}
Standard Answer (Expected Findings): {standard_answer}
Student's Answer: {student_answer}
Evaluation Criteria (Total 100 points):

Identification of Vulnerabilities (40 points):

Excellent (36-40): Correctly identifies all critical vulnerabilities and potential issues in the code.
Good (28-35): Identifies most critical vulnerabilities with minor omissions.
Fair (20-27): Identifies some vulnerabilities but misses key issues.
Poor (0-19): Fails to identify significant vulnerabilities; incorrect findings.
Accuracy of Explanations (30 points):

Excellent (27-30): Provides precise and accurate explanations for each identified issue, demonstrating deep understanding.
Good (21-26): Explanations are generally accurate with minor errors.
Fair (15-20): Explanations show basic understanding but lack depth or contain errors.
Poor (0-14): Explanations are inaccurate or demonstrate misunderstanding.
Depth of Security Insight (20 points):

Excellent (18-20): Demonstrates advanced security insights, including potential exploits and mitigation strategies.
Good (14-17): Shows good understanding with some advanced insights.
Fair (10-13): Limited insights; sticks to basic observations.
Poor (0-9): Lacks security insight; superficial analysis.
Professionalism and Clarity (10 points):

Excellent (9-10): Presents findings clearly and professionally, with well-structured arguments.
Good (7-8): Generally clear presentation with minor issues.
Fair (5-6): Clarity is compromised by organizational or language issues.
Poor (0-4): Poorly presented; hard to follow.
Instructions:

Provide a specific score for each criterion based on the student's performance.
Justify each score with concise feedback, specifically referencing the code and student's analysis.
Be meticulous and precise, ensuring that all feedback is accurate and helpful.
Avoid bias and base your assessment solely on the content provided.
Response Format:
Total Score: X/100

1. Identification of Vulnerabilities (X/40):
[Your feedback here]

2. Accuracy of Explanations (X/30):
[Your feedback here]

3. Depth of Security Insight (X/20):
[Your feedback here]

4. Professionalism and Clarity (X/10):
[Your feedback here]

**Overall Comments:**
[Summarize the student's performance, highlighting key strengths and areas for improvement.]


"""

CONFIG["SCORING_PROMPT_CODING"] = """**Role: You are an experienced smart contract auditor specializing in blockchain security. Your task is to comprehensively evaluate the student's smart contract code based on the provided standard answer and reference code (if available).**

**Materials:**

- **Question:** {question}
- **Reference Code (if provided):** {code}
- **Standard Answer:** {standard_answer}
- **Student's Answer:** {student_answer}

**Scoring Criteria:**

If the **Standard Answer** includes explicit scoring guidelines, please adhere strictly to those guidelines. 

If the **Standard Answer** does not provide specific scoring criteria, use the following default scoring guidelines:

**Functionality (40 points):**
- **Completeness (20 points):** Implements all required features and functionalities.
- **Correctness (20 points):** Code executes without errors and meets the problem requirements.

**Security (20 points):**
- **Vulnerability Assessment (10 points):** Identifies and mitigates potential security vulnerabilities.
- **Security Best Practices (10 points):** Adheres to industry-standard security practices in smart contract development.

**Gas Optimization and Efficiency (20 points):**
- **Gas Optimization (10 points):** Efficient use of gas, minimizing unnecessary operations.
- **Code Efficiency (10 points):** Optimizes performance and reduces computational overhead.

**Code Quality (10 points):**
- **Readability (5 points):** Code is well-organized, with clear variable names and structure.
- **Maintainability (5 points):** Code is easy to maintain and extend, with appropriate documentation and comments.

**Others (10 points):**
- **Innovation (5 points):** Demonstrates creative and effective solutions beyond the basic requirements.
- **Consistency (5 points):** Maintains consistent coding style and follows best practices throughout.

**Scoring Guidelines:**
- Assign specific points within each category based on the student's performance.
- Provide concise feedback for each category, highlighting strengths and areas for improvement.
- Ensure objectivity and adherence to the criteria without bias.

**Feedback Format:**
Total Score: X/100

**Detailed Scoring:**
- **Functionality (X/40):** [Feedback]
  - **Completeness (X/20):** [Feedback]
  - **Correctness (X/20):** [Feedback]
- **Security (X/20):** [Feedback]
  - **Vulnerability Assessment (X/10):** [Feedback]
  - **Security Best Practices (X/10):** [Feedback]
- **Gas Optimization and Efficiency (X/20):** [Feedback]
  - **Gas Optimization (X/10):** [Feedback]
  - **Code Efficiency (X/10):** [Feedback]
- **Code Quality (X/10):** [Feedback]
  - **Readability (X/5):** [Feedback]
  - **Maintainability (X/5):** [Feedback]
- **Others (X/10):** [Feedback]
  - **Innovation (X/5):** [Feedback]
  - **Consistency (X/5):** [Feedback]

**Overall Comments:**
[Provide a summary of the student's performance, emphasizing key strengths and areas that need improvement. Keep the comments concise and focused.]
"""


# Create semaphores for concurrency
API_SEMAPHORE = Semaphore(CONFIG["MAX_CONCURRENT_API_CALLS"])
SCORING_SEMAPHORE = Semaphore(CONFIG["MAX_CONCURRENT_SCORING"])

# Initialize a ThreadPoolExecutor
EXECUTOR = ThreadPoolExecutor(max_workers=CONFIG["MAX_CONCURRENT_API_CALLS"])

# Initialize the Anthropic client once
ANTHROPIC_CLIENT = anthropic.Anthropic(api_key=CONFIG['CLAUDE_API_KEY'])

def load_answers_from_csv(filename):
    results = []
    try:
        with open(filename, 'r', newline='', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                # Ensure all required fields are present
                for field in  ["Question", "Code", "Standard Answer", "LLM Answer"]:
                    if field not in row:
                        row[field] = ''
                # Escape special characters
                for field in ["Question", "Code", "Standard Answer", "LLM Answer"]:
                    row[field] = escape_special_chars(row[field])
                results.append(row)
        logging.info(f"Loaded {len(results)} answers from {filename}")
        return results
    except Exception as e:
        logging.error(f"Error reading answers from {filename}: {e}")
    return []



def escape_braces(text):
    return text.replace('{', '{{').replace('}', '}}')


def escape_special_chars(text):
    """Escape special characters in the text."""
    return html.escape(text).replace('\n', '\\n').replace('\r', '\\r')

async def call_claude_api(prompt, model=CONFIG["SCORING_LLM_MODEL"], max_retries=5, delay=8):
    async with API_SEMAPHORE:
        loop = asyncio.get_event_loop()
        for attempt in range(max_retries):
            try:
                # Run the synchronous API call in a thread pool executor
                response = await loop.run_in_executor(
                    EXECUTOR,
                    lambda: ANTHROPIC_CLIENT.messages.create(
                        model=model,
                        max_tokens=1000,
                        temperature=0,
                        messages=[
                            {"role": "user", "content": prompt}
                        ]
                    )
                )
                if response.content:
                    # Assuming response.content is a list and the first item has 'text'
                    return response.content[0].text.strip()
                else:
                    raise ValueError("Empty response from Claude API")
            except Exception as e:
                logging.error(f"Claude API error (attempt {attempt + 1}/{max_retries}): {str(e)}")
                if "rate limit" in str(e).lower():
                    logging.warning(f"Rate limit hit, retrying in {delay} seconds...")
                    await asyncio.sleep(delay)
                elif attempt == max_retries - 1:
                    raise
                else:
                    await asyncio.sleep(delay)
    return None

def extract_score(score_text):
    # Extract total score from the scoring LLM's response
    score_match = re.search(r'Total Score:\s*(\d{1,3})/100', score_text)
    
    if score_match:
        score = int(score_match.group(1))
        # Ensure the score is between 0 and 100
        score = max(0, min(score, 100))
    else:
        logging.warning(f"Failed to extract score from: {score_text}")
        score = 0  # Default to 0 if unable to parse

    # Use the entire response as justification
    justification = score_text.strip()
    
    return score, justification




async def score_answer(answer_data):
    # Escape braces in answers
    standard_answer = escape_braces(answer_data['Standard Answer'])
    student_answer = escape_braces(answer_data['LLM Answer'])
    question = escape_braces(answer_data['Question'])
    code = escape_braces(answer_data.get('Code', ''))  # Get code if available
    category = answer_data.get('Category', '').lower()

    if category == 'auditing':
        prompt_template = CONFIG["SCORING_PROMPT_AUDITING"]
    elif category == 'coding':
        prompt_template = CONFIG["SCORING_PROMPT_CODING"]
    else:
        prompt_template = CONFIG["SCORING_PROMPT_QA"]

    # Prepare the scoring prompt
    prompt = prompt_template.format(
        question=question,
        code=code,
        standard_answer=standard_answer,
        student_answer=student_answer
    )

    # Call the LLM for scoring
    score_text = await call_claude_api(prompt)
    if score_text is None:
        logging.error("Failed to get a response from LLM")
        return 0, "Failed to score due to API error"

    # Extract the score from the response
    score, justification = extract_score(score_text)
    return score, justification


async def process_answers(answers):
    tasks = []
    for answer_data in answers:
        task = asyncio.create_task(process_single_answer_with_semaphore(answer_data))
        tasks.append(task)
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Handle exceptions in results
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logging.error(f"Error processing answer {i}: {result}")
            # You can decide how to handle the failed task here
            # For example, you can create a default failed result
            answer_data = answers[i]
            answer_data["Score"] = 0
            answer_data["Justification"] = f"Failed to score due to error: {result}"
            processed_results.append(answer_data)
        else:
            processed_results.append(result)
    
    return processed_results

async def process_single_answer_with_semaphore(answer_data):
    async with SCORING_SEMAPHORE:
        return await process_single_answer(answer_data)

async def process_single_answer(answer_data):
    try:
        start_time = asyncio.get_event_loop().time()
        logging.info(f"Started scoring Answer for Question {answer_data.get('Question ID', '')} by Model {answer_data.get('Model', '')}")
    
        score, justification = await score_answer(answer_data)
        logging.info(f"Score: {score}")
        logging.info(f"Justification: {justification}")
    
        # Update the answer_data with the score and justification
        answer_data["Score"] = score
        answer_data["Justification"] = escape_special_chars(justification)
    
        end_time = asyncio.get_event_loop().time()
        logging.info(f"Finished scoring Answer for Question {answer_data.get('Question ID', '')} by Model {answer_data.get('Model', '')}. Took {end_time - start_time} seconds.")
        return answer_data
    except Exception as e:
        logging.error(f"Exception while processing answer for Question {answer_data.get('Question ID', '')}: {e}")
        # Assign default values in case of failure
        answer_data["Score"] = 0
        answer_data["Justification"] = f"Failed to score due to error: {e}"
        return answer_data


def save_results_to_csv(results, filename):
    fieldnames = [
        "Question ID", "Model", "Category", "Topics", "Score", "Justification",
        "Question", "Code", "Standard Answer", "LLM Answer"
    ]
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
        writer.writeheader()
        for result in results:
            # Unescape the special characters for writing to CSV
            for field in ["Question", "Code", "Standard Answer", "LLM Answer", "Justification"]:
                if field in result:
                    result[field] = html.unescape(result[field]).replace('\\n', '\n').replace('\\r', '\r')
            writer.writerow(result)
    logging.info(f"Results have been saved to {filename}")

async def main():
    try:
        answers = load_answers_from_csv(CONFIG["ANSWERS_CSV"])
        if not answers:
            logging.error("No answers loaded, exiting.")
            return

        results = await process_answers(answers)

        # Save results to CSV
        save_results_to_csv(results, CONFIG["OUTPUT_CSV"])

        logging.info("\nAll processes completed successfully.")

    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}")
        import traceback
        logging.error(traceback.format_exc())
    finally:
        EXECUTOR.shutdown(wait=True)

if __name__ == "__main__":
    asyncio.run(main())