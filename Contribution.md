# LLM Benchmark Dataset: Contribution Guidelines

## 1. Project Overview

Our project aims to create a comprehensive benchmark dataset for evaluating Large Language Models (LLMs) in the domain of cryptography and blockchain technology. We welcome contributions from the community to enhance the quality and diversity of our dataset.

## 2. How to Contribute

### 2.1 Preparing Your Contribution

- Familiarize yourself with the existing questions and categories in our dataset.
- Identify areas where you can add value based on your expertise.
- Ensure your questions are original and not duplicates of existing ones.

### 2.2 Submission Process

1. Fork the repository to your GitHub account.
2. Create a new branch for your contribution.
3. Add your questions/tasks in the appropriate category folder.
4. Commit your changes with a clear, descriptive message.
5. Open a Pull Request (PR) to our main repository.

### 2.3 Question Format

Use the following template for each question:

```markdown
## Question ID: [Leave blank, will be assigned]
**Category:** [e.g., Cryptography, Blockchain, Smart Contracts]
**Difficulty:** [Easy, Medium, Hard]
**Type:** [Multiple Choice, Open-ended, Coding Task]

**Question:**
[Your question text here]

**Options:** (for multiple choice)
A. [Option A]
B. [Option B]
C. [Option C]
D. [Option D]

**Correct Answer:** [For multiple choice, indicate the correct option]

**Explanation:**
[Provide a brief explanation of the correct answer]

**References:** (optional)
- [Any relevant links or sources]

**Contributor:** [Your GitHub username]
```

## 3. Question Criteria

- **Relevance:** Must relate to cryptography, blockchain, or associated technologies.
- **Clarity:** Questions should be unambiguous and well-formulated.
- **Difficulty:** Accurately represent the stated difficulty level.
- **Educational Value:** Should test understanding, not just memorization.
- **Practical Application:** Prefer questions that reflect real-world scenarios or applications.

## 4. Review Process

1. **Initial Check:** Maintainers will review for basic criteria compliance within 3 days.
2. **Community Feedback:** A 5-day period for community members to comment on the PR.
3. **Final Review:** Core team makes the final decision based on criteria and feedback.
4. **Iteration:** If changes are needed, contributors will be notified to update their submission.

## 5. Quality Control

- We use a community-based rating system (1-5 stars) for ongoing quality assessment.
- Questions consistently rated below 3 stars will be flagged for review or removal.
- Periodic audits will be conducted to ensure overall dataset quality.

## 6. Recognition and Rewards

- Top contributors will be featured in our README and documentation.
- Quarterly acknowledgments for the most valuable contributions.
- All contributors will be credited in dataset release notes.

## 7. Community Engagement

- Bi-monthly virtual meetups to discuss dataset improvements and trends.
- Open forum for suggesting new categories or question types.

## 8. Ethical Considerations

- Ensure questions do not promote harmful or illegal activities.
- Avoid bias in question formulation and answer options.
- Respect intellectual property rights; do not use copyrighted material without permission.

## 9. Support and Communication

- For questions or support, open an issue with the tag 'contribution-help'.
- Join our Discord channel for real-time discussions and community support.
- Check our FAQ section before asking questions.

We appreciate your contributions to improving our LLM Benchmark Dataset!
