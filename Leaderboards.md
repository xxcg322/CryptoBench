# CryptoBench Leaderboard 2024.10.02

| Rank | Model | Overall Score | coding | problem solving | system design | Computation | auditing | knowledge |
|------|-------|---------------|-----|-----|-----|-----|-----|-----|
| 1 | o1-preview | 87.85 | 92.15 | 85.56 | 87.69 | 83.69 | 90.67 | 87.35 |
| 2 | o1-mini | 87.24 | 92.85 | 87.87 | 88.38 | 79.62 | 89.86 | 84.85 |
| 3 | claude-3-5-sonnet-20240620 | 85.65 | 89.85 | 84.69 | 86.54 | 82.25 | 86.29 | 84.28 |
| 4 | Qwen2.5-72B-Instruct | 84.83 | 90.31 | 84.68 | 86.97 | 81.38 | 83.12 | 82.5 |
| 5 | gpt-4-turbo-2024-04-09 | 84.42 | 86.77 | 84.58 | 85.11 | 80.62 | 84.93 | 84.53 |
| 6 | gpt-4o-2024-08-06 | 84.32 | 86.15 | 84.49 | 85.96 | 81.62 | 83.19 | 84.53 |
| 7 | DeepSeek-V2.5 | 84.15 | 87.15 | 85.36 | 86.05 | 80.12 | 83.88 | 82.35 |
| 8 | claude-3-opus-20240229 | 83.77 | 87.85 | 83.44 | 85.57 | 81.06 | 82.36 | 82.33 |
| 9 | Meta-Llama-3.1-405B-Instruct | 83.31 | 83.15 | 85.29 | 86.5 | 82.38 | 78.52 | 84.0 |
| 10 | gpt-4o-mini-2024-07-18 | 82.76 | 87.69 | 84.2 | 83.99 | 74.94 | 83.88 | 81.88 |
| 11 | Meta-Llama-3.1-70B-Instruct | 82.37 | 83.62 | 84.87 | 85.8 | 79.44 | 77.45 | 83.05 |
| 12 | gemma-2-27b-it | 80.33 | 84.0 | 82.38 | 84.31 | 74.38 | 77.6 | 79.28 |
| 13 | claude-3-haiku-20240307 | 80.29 | 82.54 | 81.4 | 82.46 | 78.31 | 78.55 | 78.45 |
| 14 | gemini-1.5-flash | 80.15 | 85.23 | 81.76 | 82.77 | 76.69 | 75.62 | 78.83 |
| 15 | gemini-1.5-pro | 79.8 | 65.23 | 84.36 | 84.7 | 82.12 | 81.19 | 81.22 |
| 16 | mistral-nemo-12b-instruct-2407 | 79.42 | 78.23 | 82.62 | 84.85 | 75.0 | 76.0 | 79.85 |
| 17 | Mixtral-8x7B-Instruct-v0.1 | 77.0 | 75.69 | 81.87 | 81.68 | 71.5 | 71.29 | 79.95 |
| 18 | gemma-2-9b-it | 75.99 | 80.23 | 78.02 | 78.92 | 70.19 | 72.55 | 76.03 |
| 19 | Meta-Llama-3.1-8B-Instruct | 75.95 | 74.31 | 79.56 | 83.11 | 75.0 | 66.79 | 76.95 |
| 20 | Qwen2-Math-72B-Instruct | 75.92 | 60.92 | 81.93 | 82.34 | 83.06 | 71.71 | 75.58 |
| 21 | gpt-3.5-turbo-0125 | 72.33 | 78.23 | 73.84 | 75.43 | 65.5 | 70.07 | 70.92 |
| 22 | phi3-14b-medium-128k-instruct | 70.19 | 64.92 | 74.6 | 75.96 | 70.25 | 62.38 | 73.03 |
| 23 | llama3.2:3b-instruct-q8_0 | 68.36 | 69.77 | 70.78 | 76.93 | 60.69 | 63.64 | 68.33 |
| 24 | nous-hermes2:10.7b-solar-fp16 | 67.49 | 65.08 | 71.04 | 76.31 | 57.56 | 67.57 | 67.4 |


## Layer 2 Solutions Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | gpt-4o-2024-08-06 | 80.83 |
| 2 | Meta-Llama-3.1-70B-Instruct | 80.17 |
| 3 | o1-mini | 80.0 |
| 4 | Meta-Llama-3.1-405B-Instruct | 79.17 |
| 5 | Qwen2.5-72B-Instruct | 77.83 |
| 6 | o1-preview | 77.33 |
| 7 | gemma-2-27b-it | 77.33 |
| 8 | gpt-4-turbo-2024-04-09 | 77.0 |
| 9 | gemini-1.5-pro | 76.83 |
| 10 | claude-3-5-sonnet-20240620 | 76.67 |
| 11 | DeepSeek-V2.5 | 76.67 |
| 12 | claude-3-haiku-20240307 | 76.0 |
| 13 | Mixtral-8x7B-Instruct-v0.1 | 75.0 |
| 14 | gemini-1.5-flash | 74.67 |
| 15 | gemma-2-9b-it | 73.33 |
| 16 | mistral-nemo-12b-instruct-2407 | 72.83 |
| 17 | gpt-4o-mini-2024-07-18 | 71.83 |
| 18 | Meta-Llama-3.1-8B-Instruct | 71.17 |
| 19 | claude-3-opus-20240229 | 69.33 |
| 20 | Qwen2-Math-72B-Instruct | 69.17 |
| 21 | phi3-14b-medium-128k-instruct | 66.67 |
| 22 | nous-hermes2:10.7b-solar-fp16 | 64.33 |
| 23 | gpt-3.5-turbo-0125 | 63.33 |
| 24 | llama3.2:3b-instruct-q8_0 | 53.83 |


## Tokenomics Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | o1-preview | 92.5 |
| 2 | o1-mini | 91.17 |
| 3 | Qwen2.5-72B-Instruct | 89.58 |
| 4 | claude-3-5-sonnet-20240620 | 89.58 |
| 5 | DeepSeek-V2.5 | 89.25 |
| 6 | gemma-2-27b-it | 88.83 |
| 7 | Meta-Llama-3.1-70B-Instruct | 88.75 |
| 8 | Meta-Llama-3.1-405B-Instruct | 88.5 |
| 9 | claude-3-opus-20240229 | 88.0 |
| 10 | mistral-nemo-12b-instruct-2407 | 87.75 |
| 11 | gpt-4o-2024-08-06 | 87.67 |
| 12 | gemini-1.5-pro | 87.42 |
| 13 | gemini-1.5-flash | 87.33 |
| 14 | gpt-4-turbo-2024-04-09 | 86.92 |
| 15 | gpt-4o-mini-2024-07-18 | 86.83 |
| 16 | claude-3-haiku-20240307 | 86.25 |
| 17 | gemma-2-9b-it | 86.25 |
| 18 | Meta-Llama-3.1-8B-Instruct | 85.67 |
| 19 | Qwen2-Math-72B-Instruct | 84.67 |
| 20 | Mixtral-8x7B-Instruct-v0.1 | 83.92 |
| 21 | gpt-3.5-turbo-0125 | 81.25 |
| 22 | llama3.2:3b-instruct-q8_0 | 81.17 |
| 23 | nous-hermes2:10.7b-solar-fp16 | 79.67 |
| 24 | phi3-14b-medium-128k-instruct | 70.5 |


## Zero-Knowledge Proof Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | Meta-Llama-3.1-405B-Instruct | 87.33 |
| 2 | Qwen2.5-72B-Instruct | 83.92 |
| 3 | gemini-1.5-pro | 83.67 |
| 4 | o1-mini | 83.67 |
| 5 | Meta-Llama-3.1-70B-Instruct | 82.92 |
| 6 | gpt-4-turbo-2024-04-09 | 82.08 |
| 7 | claude-3-5-sonnet-20240620 | 81.92 |
| 8 | claude-3-haiku-20240307 | 81.67 |
| 9 | gpt-4o-2024-08-06 | 81.42 |
| 10 | gpt-4o-mini-2024-07-18 | 81.33 |
| 11 | Qwen2-Math-72B-Instruct | 81.17 |
| 12 | claude-3-opus-20240229 | 80.92 |
| 13 | o1-preview | 80.83 |
| 14 | DeepSeek-V2.5 | 80.75 |
| 15 | gemini-1.5-flash | 80.67 |
| 16 | gemma-2-27b-it | 80.5 |
| 17 | mistral-nemo-12b-instruct-2407 | 80.08 |
| 18 | Mixtral-8x7B-Instruct-v0.1 | 79.42 |
| 19 | phi3-14b-medium-128k-instruct | 79.17 |
| 20 | Meta-Llama-3.1-8B-Instruct | 78.92 |
| 21 | gemma-2-9b-it | 76.58 |
| 22 | llama3.2:3b-instruct-q8_0 | 73.42 |
| 23 | gpt-3.5-turbo-0125 | 71.5 |
| 24 | nous-hermes2:10.7b-solar-fp16 | 69.33 |


## DAO & Governance Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | o1-mini | 90.96 |
| 2 | claude-3-5-sonnet-20240620 | 90.0 |
| 3 | Meta-Llama-3.1-405B-Instruct | 90.0 |
| 4 | o1-preview | 89.57 |
| 5 | Qwen2.5-72B-Instruct | 89.43 |
| 6 | claude-3-opus-20240229 | 89.3 |
| 7 | gpt-4o-2024-08-06 | 88.87 |
| 8 | DeepSeek-V2.5 | 88.78 |
| 9 | mistral-nemo-12b-instruct-2407 | 88.7 |
| 10 | gemini-1.5-pro | 88.39 |
| 11 | Meta-Llama-3.1-70B-Instruct | 88.3 |
| 12 | gemma-2-27b-it | 87.96 |
| 13 | gemini-1.5-flash | 87.87 |
| 14 | Qwen2-Math-72B-Instruct | 87.7 |
| 15 | gpt-4-turbo-2024-04-09 | 87.61 |
| 16 | gpt-4o-mini-2024-07-18 | 87.52 |
| 17 | Meta-Llama-3.1-8B-Instruct | 87.09 |
| 18 | gemma-2-9b-it | 86.09 |
| 19 | claude-3-haiku-20240307 | 85.7 |
| 20 | Mixtral-8x7B-Instruct-v0.1 | 85.61 |
| 21 | llama3.2:3b-instruct-q8_0 | 81.35 |
| 22 | nous-hermes2:10.7b-solar-fp16 | 81.09 |
| 23 | gpt-3.5-turbo-0125 | 80.65 |
| 24 | phi3-14b-medium-128k-instruct | 75.17 |


## Security Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | o1-mini | 88.0 |
| 2 | o1-preview | 87.59 |
| 3 | claude-3-5-sonnet-20240620 | 85.59 |
| 4 | DeepSeek-V2.5 | 84.0 |
| 5 | gpt-4-turbo-2024-04-09 | 83.57 |
| 6 | gpt-4o-2024-08-06 | 83.27 |
| 7 | Qwen2.5-72B-Instruct | 82.9 |
| 8 | gpt-4o-mini-2024-07-18 | 82.88 |
| 9 | claude-3-opus-20240229 | 82.65 |
| 10 | gemini-1.5-pro | 82.28 |
| 11 | Meta-Llama-3.1-405B-Instruct | 81.78 |
| 12 | Meta-Llama-3.1-70B-Instruct | 81.71 |
| 13 | claude-3-haiku-20240307 | 79.86 |
| 14 | gemma-2-27b-it | 79.53 |
| 15 | gemini-1.5-flash | 78.99 |
| 16 | mistral-nemo-12b-instruct-2407 | 78.78 |
| 17 | Mixtral-8x7B-Instruct-v0.1 | 77.01 |
| 18 | Qwen2-Math-72B-Instruct | 76.7 |
| 19 | Meta-Llama-3.1-8B-Instruct | 74.62 |
| 20 | gemma-2-9b-it | 74.08 |
| 21 | gpt-3.5-turbo-0125 | 71.31 |
| 22 | phi3-14b-medium-128k-instruct | 69.24 |
| 23 | llama3.2:3b-instruct-q8_0 | 69.09 |
| 24 | nous-hermes2:10.7b-solar-fp16 | 68.8 |


## Bitcoin Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | gpt-4o-2024-08-06 | 85.76 |
| 2 | o1-mini | 85.59 |
| 3 | Qwen2.5-72B-Instruct | 84.12 |
| 4 | o1-preview | 84.12 |
| 5 | claude-3-5-sonnet-20240620 | 83.41 |
| 6 | Meta-Llama-3.1-405B-Instruct | 83.41 |
| 7 | Meta-Llama-3.1-70B-Instruct | 82.94 |
| 8 | gpt-4-turbo-2024-04-09 | 82.82 |
| 9 | DeepSeek-V2.5 | 82.29 |
| 10 | gemini-1.5-pro | 80.94 |
| 11 | gemma-2-27b-it | 80.82 |
| 12 | claude-3-opus-20240229 | 80.53 |
| 13 | gemini-1.5-flash | 80.41 |
| 14 | gpt-4o-mini-2024-07-18 | 80.29 |
| 15 | claude-3-haiku-20240307 | 79.18 |
| 16 | gemma-2-9b-it | 79.12 |
| 17 | mistral-nemo-12b-instruct-2407 | 78.82 |
| 18 | Mixtral-8x7B-Instruct-v0.1 | 78.65 |
| 19 | Qwen2-Math-72B-Instruct | 78.47 |
| 20 | Meta-Llama-3.1-8B-Instruct | 75.88 |
| 21 | phi3-14b-medium-128k-instruct | 73.47 |
| 22 | gpt-3.5-turbo-0125 | 72.94 |
| 23 | nous-hermes2:10.7b-solar-fp16 | 68.18 |
| 24 | llama3.2:3b-instruct-q8_0 | 64.82 |


## DeFi Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | o1-mini | 91.53 |
| 2 | o1-preview | 90.83 |
| 3 | Qwen2.5-72B-Instruct | 87.92 |
| 4 | claude-3-5-sonnet-20240620 | 87.89 |
| 5 | DeepSeek-V2.5 | 87.25 |
| 6 | gpt-4o-2024-08-06 | 87.17 |
| 7 | gpt-4o-mini-2024-07-18 | 87.17 |
| 8 | claude-3-opus-20240229 | 86.56 |
| 9 | gpt-4-turbo-2024-04-09 | 86.5 |
| 10 | Meta-Llama-3.1-405B-Instruct | 85.39 |
| 11 | gemini-1.5-flash | 85.33 |
| 12 | gemma-2-27b-it | 85.14 |
| 13 | claude-3-haiku-20240307 | 84.81 |
| 14 | gemini-1.5-pro | 83.89 |
| 15 | Meta-Llama-3.1-70B-Instruct | 83.56 |
| 16 | mistral-nemo-12b-instruct-2407 | 83.44 |
| 17 | gemma-2-9b-it | 83.36 |
| 18 | Mixtral-8x7B-Instruct-v0.1 | 82.86 |
| 19 | Meta-Llama-3.1-8B-Instruct | 81.14 |
| 20 | Qwen2-Math-72B-Instruct | 77.83 |
| 21 | phi3-14b-medium-128k-instruct | 77.11 |
| 22 | gpt-3.5-turbo-0125 | 76.92 |
| 23 | llama3.2:3b-instruct-q8_0 | 75.47 |
| 24 | nous-hermes2:10.7b-solar-fp16 | 73.39 |


## Consensus Mechanisms Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | gpt-4o-2024-08-06 | 85.11 |
| 2 | claude-3-opus-20240229 | 84.78 |
| 3 | o1-preview | 84.67 |
| 4 | DeepSeek-V2.5 | 83.0 |
| 5 | Qwen2.5-72B-Instruct | 82.44 |
| 6 | gpt-4-turbo-2024-04-09 | 82.33 |
| 7 | Mixtral-8x7B-Instruct-v0.1 | 82.0 |
| 8 | Qwen2-Math-72B-Instruct | 82.0 |
| 9 | gemma-2-27b-it | 81.78 |
| 10 | claude-3-haiku-20240307 | 81.56 |
| 11 | gemini-1.5-pro | 81.44 |
| 12 | o1-mini | 81.44 |
| 13 | Meta-Llama-3.1-70B-Instruct | 81.44 |
| 14 | Meta-Llama-3.1-405B-Instruct | 80.67 |
| 15 | mistral-nemo-12b-instruct-2407 | 80.0 |
| 16 | claude-3-5-sonnet-20240620 | 79.67 |
| 17 | gpt-4o-mini-2024-07-18 | 79.0 |
| 18 | gemini-1.5-flash | 75.33 |
| 19 | Meta-Llama-3.1-8B-Instruct | 75.33 |
| 20 | gpt-3.5-turbo-0125 | 74.11 |
| 21 | gemma-2-9b-it | 73.89 |
| 22 | nous-hermes2:10.7b-solar-fp16 | 70.33 |
| 23 | llama3.2:3b-instruct-q8_0 | 70.22 |
| 24 | phi3-14b-medium-128k-instruct | 66.33 |


## Blockchain Fundamental Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | o1-mini | 88.52 |
| 2 | o1-preview | 87.19 |
| 3 | gpt-4o-2024-08-06 | 86.05 |
| 4 | Meta-Llama-3.1-70B-Instruct | 85.71 |
| 5 | gpt-4-turbo-2024-04-09 | 85.62 |
| 6 | Qwen2.5-72B-Instruct | 85.0 |
| 7 | DeepSeek-V2.5 | 84.62 |
| 8 | claude-3-5-sonnet-20240620 | 84.52 |
| 9 | Meta-Llama-3.1-405B-Instruct | 84.52 |
| 10 | claude-3-opus-20240229 | 84.29 |
| 11 | gpt-4o-mini-2024-07-18 | 83.52 |
| 12 | mistral-nemo-12b-instruct-2407 | 83.48 |
| 13 | gemma-2-27b-it | 83.43 |
| 14 | gemini-1.5-flash | 82.57 |
| 15 | Qwen2-Math-72B-Instruct | 81.67 |
| 16 | Meta-Llama-3.1-8B-Instruct | 81.62 |
| 17 | claude-3-haiku-20240307 | 80.24 |
| 18 | Mixtral-8x7B-Instruct-v0.1 | 79.0 |
| 19 | gemma-2-9b-it | 77.81 |
| 20 | gemini-1.5-pro | 77.62 |
| 21 | gpt-3.5-turbo-0125 | 72.38 |
| 22 | phi3-14b-medium-128k-instruct | 72.38 |
| 23 | llama3.2:3b-instruct-q8_0 | 70.62 |
| 24 | nous-hermes2:10.7b-solar-fp16 | 70.62 |


## Cryptography Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | Meta-Llama-3.1-405B-Instruct | 86.96 |
| 2 | Qwen2.5-72B-Instruct | 86.47 |
| 3 | o1-preview | 86.44 |
| 4 | gpt-4-turbo-2024-04-09 | 85.87 |
| 5 | o1-mini | 85.58 |
| 6 | Meta-Llama-3.1-70B-Instruct | 85.56 |
| 7 | gpt-4o-2024-08-06 | 85.09 |
| 8 | claude-3-5-sonnet-20240620 | 84.73 |
| 9 | DeepSeek-V2.5 | 84.62 |
| 10 | gpt-4o-mini-2024-07-18 | 82.67 |
| 11 | claude-3-opus-20240229 | 82.62 |
| 12 | gemini-1.5-pro | 82.49 |
| 13 | mistral-nemo-12b-instruct-2407 | 81.96 |
| 14 | Qwen2-Math-72B-Instruct | 81.4 |
| 15 | gemini-1.5-flash | 80.24 |
| 16 | gemma-2-27b-it | 80.18 |
| 17 | claude-3-haiku-20240307 | 80.11 |
| 18 | Mixtral-8x7B-Instruct-v0.1 | 78.44 |
| 19 | Meta-Llama-3.1-8B-Instruct | 77.29 |
| 20 | gemma-2-9b-it | 75.8 |
| 21 | phi3-14b-medium-128k-instruct | 75.78 |
| 22 | gpt-3.5-turbo-0125 | 72.8 |
| 23 | llama3.2:3b-instruct-q8_0 | 68.6 |
| 24 | nous-hermes2:10.7b-solar-fp16 | 67.69 |


## Smart Contract Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | o1-mini | 88.42 |
| 2 | o1-preview | 88.29 |
| 3 | claude-3-5-sonnet-20240620 | 85.51 |
| 4 | gpt-4-turbo-2024-04-09 | 83.45 |
| 5 | Qwen2.5-72B-Instruct | 83.09 |
| 6 | DeepSeek-V2.5 | 83.03 |
| 7 | gpt-4o-2024-08-06 | 82.76 |
| 8 | gpt-4o-mini-2024-07-18 | 82.76 |
| 9 | claude-3-opus-20240229 | 82.56 |
| 10 | Meta-Llama-3.1-405B-Instruct | 80.79 |
| 11 | Meta-Llama-3.1-70B-Instruct | 80.04 |
| 12 | gemini-1.5-pro | 79.15 |
| 13 | claude-3-haiku-20240307 | 78.79 |
| 14 | gemma-2-27b-it | 78.35 |
| 15 | mistral-nemo-12b-instruct-2407 | 77.47 |
| 16 | gemini-1.5-flash | 77.27 |
| 17 | Mixtral-8x7B-Instruct-v0.1 | 74.95 |
| 18 | gemma-2-9b-it | 72.83 |
| 19 | Meta-Llama-3.1-8B-Instruct | 72.38 |
| 20 | Qwen2-Math-72B-Instruct | 72.11 |
| 21 | gpt-3.5-turbo-0125 | 70.64 |
| 22 | nous-hermes2:10.7b-solar-fp16 | 67.42 |
| 23 | phi3-14b-medium-128k-instruct | 67.18 |
| 24 | llama3.2:3b-instruct-q8_0 | 66.96 |


## Agent Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | Meta-Llama-3.1-70B-Instruct | 91.46 |
| 2 | Meta-Llama-3.1-405B-Instruct | 90.92 |
| 3 | claude-3-5-sonnet-20240620 | 90.38 |
| 4 | claude-3-opus-20240229 | 90.38 |
| 5 | gemini-1.5-pro | 90.31 |
| 6 | gemma-2-27b-it | 90.08 |
| 7 | Qwen2.5-72B-Instruct | 90.0 |
| 8 | phi3-14b-medium-128k-instruct | 90.0 |
| 9 | gemini-1.5-flash | 89.69 |
| 10 | Meta-Llama-3.1-8B-Instruct | 89.69 |
| 11 | gpt-4o-2024-08-06 | 89.62 |
| 12 | o1-mini | 89.38 |
| 13 | DeepSeek-V2.5 | 89.38 |
| 14 | mistral-nemo-12b-instruct-2407 | 88.85 |
| 15 | o1-preview | 88.69 |
| 16 | Qwen2-Math-72B-Instruct | 88.62 |
| 17 | claude-3-haiku-20240307 | 88.38 |
| 18 | Mixtral-8x7B-Instruct-v0.1 | 87.62 |
| 19 | gpt-4o-mini-2024-07-18 | 87.46 |
| 20 | gpt-4-turbo-2024-04-09 | 86.54 |
| 21 | llama3.2:3b-instruct-q8_0 | 86.15 |
| 22 | gemma-2-9b-it | 85.85 |
| 23 | nous-hermes2:10.7b-solar-fp16 | 84.23 |
| 24 | gpt-3.5-turbo-0125 | 82.85 |


## Solana Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | o1-preview | 87.27 |
| 2 | gpt-4-turbo-2024-04-09 | 85.13 |
| 3 | claude-3-5-sonnet-20240620 | 84.4 |
| 4 | claude-3-opus-20240229 | 84.33 |
| 5 | gpt-4o-2024-08-06 | 84.27 |
| 6 | DeepSeek-V2.5 | 84.07 |
| 7 | o1-mini | 83.93 |
| 8 | Qwen2.5-72B-Instruct | 83.07 |
| 9 | Meta-Llama-3.1-405B-Instruct | 82.2 |
| 10 | gemma-2-27b-it | 80.6 |
| 11 | Meta-Llama-3.1-70B-Instruct | 80.2 |
| 12 | gemini-1.5-pro | 80.13 |
| 13 | mistral-nemo-12b-instruct-2407 | 79.87 |
| 14 | Mixtral-8x7B-Instruct-v0.1 | 79.53 |
| 15 | gpt-4o-mini-2024-07-18 | 79.2 |
| 16 | claude-3-haiku-20240307 | 78.07 |
| 17 | Meta-Llama-3.1-8B-Instruct | 75.13 |
| 18 | Qwen2-Math-72B-Instruct | 74.53 |
| 19 | phi3-14b-medium-128k-instruct | 72.67 |
| 20 | gemini-1.5-flash | 72.33 |
| 21 | nous-hermes2:10.7b-solar-fp16 | 70.73 |
| 22 | gemma-2-9b-it | 69.87 |
| 23 | gpt-3.5-turbo-0125 | 69.4 |
| 24 | llama3.2:3b-instruct-q8_0 | 68.47 |


## Ethereum Leaderboard

| Rank | Model | Score |
|------|-------|-------|
| 1 | o1-preview | 88.62 |
| 2 | o1-mini | 86.67 |
| 3 | gpt-4-turbo-2024-04-09 | 84.19 |
| 4 | claude-3-5-sonnet-20240620 | 83.98 |
| 5 | Qwen2.5-72B-Instruct | 83.57 |
| 6 | gpt-4o-2024-08-06 | 82.95 |
| 7 | Meta-Llama-3.1-405B-Instruct | 82.6 |
| 8 | claude-3-opus-20240229 | 82.43 |
| 9 | DeepSeek-V2.5 | 82.05 |
| 10 | gpt-4o-mini-2024-07-18 | 81.5 |
| 11 | gemini-1.5-pro | 81.4 |
| 12 | Meta-Llama-3.1-70B-Instruct | 80.29 |
| 13 | mistral-nemo-12b-instruct-2407 | 79.64 |
| 14 | claude-3-haiku-20240307 | 78.45 |
| 15 | gemma-2-27b-it | 77.57 |
| 16 | gemini-1.5-flash | 76.88 |
| 17 | Mixtral-8x7B-Instruct-v0.1 | 75.83 |
| 18 | Meta-Llama-3.1-8B-Instruct | 74.62 |
| 19 | gemma-2-9b-it | 73.19 |
| 20 | Qwen2-Math-72B-Instruct | 73.12 |
| 21 | gpt-3.5-turbo-0125 | 69.48 |
| 22 | phi3-14b-medium-128k-instruct | 66.9 |
| 23 | nous-hermes2:10.7b-solar-fp16 | 66.33 |
| 24 | llama3.2:3b-instruct-q8_0 | 65.38 |


